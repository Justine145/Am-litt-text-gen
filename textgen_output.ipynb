{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e18ecce",
   "metadata": {},
   "source": [
    "# Text Generation with Neural Networks (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9dd8e",
   "metadata": {},
   "source": [
    "### In the previous notebook, a model has been created and trained. Here it is imported and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d2f2dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 12:37:11.607619: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 12:37:11.799938: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-07 12:37:11.799962: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-07 12:37:12.750161: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-07 12:37:12.750244: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-07 12:37:12.750254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import json\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import keras.utils\n",
    "\n",
    "from keras.utils import pad_sequences, to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ab22c",
   "metadata": {},
   "source": [
    "#### Importing the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0584cf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 12:37:15.924484: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-07 12:37:15.924513: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-07 12:37:15.924536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (justine-XPS13): /proc/driver/nvidia/version does not exist\n",
      "2023-04-07 12:37:15.924868: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "with open(\"model_amtextgen.json\", \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "\n",
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dfeafd",
   "metadata": {},
   "source": [
    "#### Importing the variables necessary to the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43625d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"max_len_ref.txt\", \"r\") as file:\n",
    "    max_len_txt = file.read()\n",
    "    max_len = int(max_len_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446bc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_ref.txt\", \"r\") as file:\n",
    "    full = file.read().split('\\n')\n",
    "    cleaned = full[:-1]                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b966bad",
   "metadata": {},
   "source": [
    "#### The model can finally be tested in this last part :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c0a1d",
   "metadata": {},
   "source": [
    "\"Seed_text\" is the text that will be continued by the model, it corresponds to the variable \"text\" and can be modified as desired. It is also possible to change the number of words to output.\n",
    "\n",
    "Before generating the text, a tokenisation is done, as well as a padding task. The model is then used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ccc0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text):\n",
    "    next_words = 50\n",
    "    max_sequence_len = max_len\n",
    "    corpus = cleaned\n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        \n",
    "        predict_x = loaded_model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predict_x,axis=1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    seed_text += \".\"\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665eb76",
   "metadata": {},
   "source": [
    "Generating text using the created function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b8d2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I opened the door and towards the light of sleeping with his middle of the room thinking it never be heard of a room light in a most days and and and there from them did my other while over the alleghanian ridge in the forecastle sign of time a rag unless i will not.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texte = \"I opened the door and\"\n",
    "generate_text(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df372da5",
   "metadata": {},
   "source": [
    "Or generating text with a nice looking API interface :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02e52d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.13.0, however version 3.14.0 is available, please upgrade.\n",
      "--------\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://c1c1f9a9ad0f8f08.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c1c1f9a9ad0f8f08.gradio.app\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(fn=generate_text, inputs=\"text\", outputs=\"text\", title= \"American literature text generator\")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157b5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
